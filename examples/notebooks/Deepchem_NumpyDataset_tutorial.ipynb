{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Deepchem Datasets\n",
    "In this tutorial we will have a look at various deepchem `dataset` methods present in `deepchem.datasets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skand/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import deepchem as dc\n",
    "import numpy as np\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using NumpyDatasets \n",
    "This is used when you have your data in numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data is your dataset in numpy array of size : 20x20.\n",
    "data = np.random.random((4, 4))\n",
    "labels = np.random.random((4,)) # labels of size 20x1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchem.data.datasets import NumpyDataset # import NumpyDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = NumpyDataset(data, labels) # creates numpy dataset object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting X, y from NumpyDataset Object\n",
    "Extracting the data and labels from the NumpyDataset is very easy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.63188616, 0.24690483, 0.85294168, 0.15512774],\n",
       "       [0.62009111, 0.00525149, 0.56082693, 0.0649767 ],\n",
       "       [0.57476389, 0.92047762, 0.36311505, 0.53421993],\n",
       "       [0.5768823 , 0.51945064, 0.9655427 , 0.82099216]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.X # Extracts the data (X) from the NumpyDataset Object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5102078 ],\n",
       "       [0.76199464],\n",
       "       [0.77398379],\n",
       "       [0.09498917]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.y # Extracts the labels (y) from the NumpyDataset Object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights of a dataset - w\n",
    "So apart from `X` and `y` which are the data and the labels, you can also assign weights `w` to each data instance. The dimension of `w` is same as that of `y`(which is Nx1 where N is the number of data instances).\n",
    "\n",
    "**NOTE:** By default `w` is a vector initialized with equal weights (all being 1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.w # printing the weights that are assigned by default. Notice that they are a vector of 1's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.random.random((4,)) # initializing weights with random vector of size 20x1\n",
    "dataset_with_weights = NumpyDataset(data, labels, w) # creates numpy dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.85432113],\n",
       "       [0.91847254],\n",
       "       [0.59774769],\n",
       "       [0.36659207]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_with_weights.w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterating over NumpyDataset\n",
    "In order to iterate over NumpyDataset, we use `itersamples` method. We iterate over 4 quantities, namely `X`, `y`, `w` and `ids`. The first three quantities are the same as discussed above and `ids` is the id of the data instance. By default the id is given in order starting from `1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0.63188616, 0.24690483, 0.85294168, 0.15512774]), array([0.5102078]), array([1.]), 0)\n",
      "(array([0.62009111, 0.00525149, 0.56082693, 0.0649767 ]), array([0.76199464]), array([1.]), 1)\n",
      "(array([0.57476389, 0.92047762, 0.36311505, 0.53421993]), array([0.77398379]), array([1.]), 2)\n",
      "(array([0.5768823 , 0.51945064, 0.9655427 , 0.82099216]), array([0.09498917]), array([1.]), 3)\n"
     ]
    }
   ],
   "source": [
    "for x, y, w, id in dataset.itersamples():\n",
    "    print(x, y, w, id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also extract the ids by `dataset.ids`. This would return a numpy array consisting of the ids of the data instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Example\n",
    "Just to get a better understanding, lets take read MNIST data and use `NumpyDataset` to store the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the numpy data of MNIST into NumpyDataset\n",
    "train = NumpyDataset(mnist.train.images, mnist.train.labels)\n",
    "valid = NumpyDataset(mnist.validation.images, mnist.validation.labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0d4ab600d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize one sample \n",
    "sample = np.reshape(train.X[5], (28, 28))\n",
    "plt.imshow(sample)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy Array to tf.data.dataset()\n",
    "This is quite similar to getting a `NumpyDataset` object from numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data\n",
      "\n",
      "[[0.78574579 0.79398959 0.64737371 0.20447343 0.55009141]\n",
      " [0.39201333 0.12299678 0.69700424 0.57494847 0.59895521]\n",
      " [0.711899   0.22786574 0.6436164  0.49713391 0.31487844]\n",
      " [0.95354154 0.67493395 0.84554228 0.15894518 0.0154379 ]]\n",
      "\n",
      " Labels\n",
      "[0.61605796 0.07695742 0.1084755  0.30322915]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "data_small = np.random.random((4,5))\n",
    "label_small = np.random.random((4,))\n",
    "dataset = tf.data.Dataset.from_tensor_slices((data_small, label_small))\n",
    "print (\"Data\\n\")\n",
    "print (data_small)\n",
    "print (\"\\n Labels\")\n",
    "print (label_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the numpy dataset from tf.data\n",
    "In order to extract the numpy array from the `tf.data`, you first need to define an `iterator` to iterate over the `tf.data.Dataset` object and then in the tensorflow session, run over the iterator to get the data instances. Let's have a look at how it's done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy Data\n",
      "[[0.78574579 0.79398959 0.64737371 0.20447343 0.55009141]\n",
      " [0.39201333 0.12299678 0.69700424 0.57494847 0.59895521]\n",
      " [0.711899   0.22786574 0.6436164  0.49713391 0.31487844]\n",
      " [0.95354154 0.67493395 0.84554228 0.15894518 0.0154379 ]]\n",
      "\n",
      " Numpy Label\n",
      "[0.61605796 0.07695742 0.1084755  0.30322915]\n"
     ]
    }
   ],
   "source": [
    "iterator = dataset.make_one_shot_iterator() # iterator\n",
    "next_element = iterator.get_next()\n",
    "numpy_data = np.zeros((4, 5))\n",
    "numpy_label = np.zeros((4,))\n",
    "sess = tf.Session() # tensorflow session \n",
    "for i in range(4):\n",
    "    data_, label_ = sess.run(next_element) # data_ contains the data and label_ contains the labels that we fed in the previous step\n",
    "    numpy_data[i, :] = data_\n",
    "    numpy_label[i] = label_\n",
    "    \n",
    "print (\"Numpy Data\")\n",
    "print(numpy_data)\n",
    "print (\"\\n Numpy Label\")\n",
    "print(numpy_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have the numpy arrays of `data` and `labels`, you can convert it to `NumpyDataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.78574579, 0.79398959, 0.64737371, 0.20447343, 0.55009141],\n",
       "       [0.39201333, 0.12299678, 0.69700424, 0.57494847, 0.59895521],\n",
       "       [0.711899  , 0.22786574, 0.6436164 , 0.49713391, 0.31487844],\n",
       "       [0.95354154, 0.67493395, 0.84554228, 0.15894518, 0.0154379 ]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_ = NumpyDataset(numpy_data, numpy_label) # convert to NumpyDataset\n",
    "dataset_.X  # printing just to check if the data is same!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting NumpyDataset to `tf.data`\n",
    "This can be easily done by the `make_iterator()` method of `NumpyDataset`. This converts the `NumpyDataset` to `tf.data`. Let's look how it's done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy Data\n",
      "[[0.78574579 0.79398959 0.64737371 0.20447343 0.55009141]\n",
      " [0.95354154 0.67493395 0.84554228 0.15894518 0.0154379 ]\n",
      " [0.711899   0.22786574 0.6436164  0.49713391 0.31487844]\n",
      " [0.39201333 0.12299678 0.69700424 0.57494847 0.59895521]]\n",
      "\n",
      " Numpy Label\n",
      "[[0.61605796]\n",
      " [0.30322915]\n",
      " [0.1084755 ]\n",
      " [0.07695742]]\n"
     ]
    }
   ],
   "source": [
    "iterator_ = dataset_.make_iterator() # Using make_iterator for converting NumpyDataset to tf.data\n",
    "next_element_ = iterator_.get_next()\n",
    "\n",
    "sess = tf.Session() # tensorflow session \n",
    "data_and_labels = sess.run(next_element_) # data_ contains the data and label_ contains the labels that we fed in the previous step\n",
    "\n",
    "\n",
    "print (\"Numpy Data\")\n",
    "print(data_and_labels[0])  # Data in the first index \n",
    "print (\"\\n Numpy Label\")\n",
    "print(data_and_labels[1])  # Labels in the second index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
